# Local-RAG: A Semantic Search Engine

## Project Description

This project implements a fully local **Retrieval-Augmented Generation (RAG)** system. The goal is to allow users to query their own documents (PDFs) using Large Language Models (LLMs) while ensuring data privacy and avoiding external API costs.

Unlike a simple chatbot, this system employs a **data engineering** approach to index content in a vector database, enabling high-precision semantic searches. It features a multi-modal ingestion pipeline that processes not only text but also tables and image captions from PDF documents.

## System Architecture

The system is divided into three main pipelines:

1.  **Data Ingestion Pipeline**:
    *   **Multi-modal Extraction**: Extracts text (`PyMuPDF`), tables (`camelot-py`), and image captions (`PyMuPDF`) from PDF files.
    *   **Chunking**: Splits the extracted content into smaller, manageable chunks using `RecursiveCharacterTextSplitter`.

2.  **Embedding & Vector Storage**:
    *   **Vectorization**: Transforms the chunks into dense vectors using the `nomic-embed-text` model.
    *   **Storage**: Stores the vectors in a persistent **ChromaDB** collection.

3.  **RAG Cycle**:
    *   **Query Transformation**: The user's question is rephrased into a concise search query optimized for vector retrieval.
    *   **Retrieval**: Fetches relevant documents from the vector DB using a Maximal Marginal Relevance (MMR) search to ensure diversity.
    *   **Re-ranking**: A `CrossEncoder` model re-ranks the retrieved documents to improve relevance.
    *   **Context Injection**: The top-ranked documents are used to build an enriched prompt.
    *   **Generation**: The final answer is generated by **Llama 3** (via Ollama) based on the provided context.

## Tech Stack

*   **Orchestration**: [LangChain](https://python.langchain.com/)
*   **LLM & Embeddings**: [Ollama](https://ollama.com/) (Llama 3, nomic-embed-text)
*   **Vector Database**: [ChromaDB](https://www.trychroma.com/)
*   **Re-ranking**: [Sentence Transformers](https://www.sbert.net/) (Cross-Encoders)
*   **PDF Processing**: PyMuPDF, camelot-py
*   **Web Interface**: [Streamlit](https://streamlit.io/)

## Installation

Follow these steps to set up and run the project locally.

**1. Prerequisites**
*   Python 3.10+
*   [Ollama](https://ollama.com/) installed and running on your local machine.
*   For table extraction with `camelot-py`, you may need to install its dependencies. On Windows, you might need to install [Ghostscript](https://www.ghostscript.com/releases/gsdnld.html). On Linux, you can use a package manager (e.g., `sudo apt-get install ghostscript python3-tk`).

**2. Clone the Repository**
```bash
git clone <your-repository-url>
cd <project-directory>
```

**3. Create a Virtual Environment and Install Dependencies**
It is recommended to use a virtual environment to manage dependencies.
```bash
# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
```
# Install the required packages
Install all dependencies using the provided `requirements.txt` file:

```bash
pip install -r requirements.txt
```

**4. Download and Install Ollama & Models**  
Install Ollama if you haven't already, then pull the required models.

*Install Ollama:*  
Follow the instructions for your OS at [Ollama's official site](https://ollama.com/download).

*Pull the models used by this application:*  
```bash
ollama pull llama3
ollama pull nomic-embed-text
```
```

## Usage

1.  **Run the Streamlit Application**
    Make sure your Ollama application is running in the background. Then, start the Streamlit app from the project's root directory:
    ```bash
    streamlit run __main__.py
    ```

2.  **Upload a PDF**
    *   The web interface will open in your browser.
    *   Use the sidebar to upload a PDF file. The system will process it, extract text, tables, and images, and store the embeddings in a new ChromaDB collection.

3.  **Select a Collection**
    *   Once the PDF is processed, its corresponding collection will appear in the "Existing Collections" dropdown in the sidebar.
    *   Select the collection you wish to query.

4.  **Ask Questions**
    *   Use the main chat input to ask questions about the content of the selected document.
    *   The RAG system will retrieve relevant information, generate an answer, and display it along with the sources used.

from ollama import Client
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

def generate_rag_response(
                          ollama_host_url = "http://localhost:11434/", 
                          local_model = "llama3", 
                          embedding_model = "mxbai-embed-large", 
                          question = None,
                          collection_name = "local_rag_db", 
                          persist_directory="./chroma_db",
                          ):
    
    """
    Function that implements a Retrieval-Augmented Generation (RAG) system.
    Given a local LLM model and a Chroma vector database, it retrieves the most relevant documents for a question,
    builds a prompt with their content as context, and sends the prompt to the LLM model to generate an answer.
    Returns the answer generated by the model.
    """

    if question is None or question.strip() == "":
        raise ValueError("A question must be provided to generate a RAG response.")

    # Initialize the Ollama client to interact with the LLM model
    client = Client(host=ollama_host_url)

    # Generate a search query from the user's question
    query_generation_prompt = f"""You are an expert at rephrasing a user's question into a concise, keyword-based search query for a vector database. 
    Extract only the most critical entities and technical terms. Do not add any explanation or introductory text.

    Example:
    User Question: "Can you tell me all about how to use AWS S3 for storing large video files?"
    Search Query: "AWS S3 large video file storage"

    ---

    User Question: "{question}"
    Search Query:"""

    response = client.chat(
        model=local_model, 
        messages=[{'role': 'user', 'content': query_generation_prompt}],
        options={'temperature': 0} # Vogliamo una risposta deterministica
    )
    search_query = response['message']['content'].strip().strip('"')
    logging.info("Generated Search Query: '%s'", search_query)

    # Create or connect to a Chroma vector database for RAG
    vector_db = Chroma(
                    persist_directory = persist_directory,
                    embedding_function=OllamaEmbeddings(model=embedding_model, base_url=ollama_host_url), 
                    collection_name = collection_name
                    )
    
    # Get a retriever to fetch relevant documents from the database
    retriever = vector_db.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 8, "fetch_k": 20}
    )

    # Retrieve the most relevant documents for the question
    docs = retriever.invoke(search_query)
    logging.info("Documents fetched from database: %d", len(docs))

    # Log the retrieved documents for debugging purposes 
    logging.info("--- CONTESTO RECUPERATO ---")
    for i, doc in enumerate(docs):
        logging.info("--- Documento %d ---\n%s\n--------------------", i+1, doc.page_content)
    logging.info("--- FINE CONTESTO ---")

    # Merge the content of the documents into a single context
    context = "\n\n".join(doc.page_content for doc in docs)

    # Create the formatted prompt for the LLM model, including only the retrieved context
    formatted_prompt = f"""Answer the question based ONLY on the following context:
    {context}
    Question: {question}
    """
    
    # Send the prompt to the LLM model and get the answer
    response = client.chat(model = local_model, messages = [{'role': 'user', 'content': formatted_prompt}])
    return(response['message']['content'])

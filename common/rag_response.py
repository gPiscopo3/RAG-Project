from ollama import Client
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings

def generate_rag_response(
                          ollama_host_url = "http://localhost:11434/", 
                          local_model = "llama3", 
                          embedding_model = "nomic-embed-text", 
                          question = None,
                          collection_name = "local_rag_db", 
                          persist_directory="./chroma_db",
                          ):
    
    """
    Function that implements a Retrieval-Augmented Generation (RAG) system.
    Given a local LLM model and a Chroma vector database, it retrieves the most relevant documents for a question,
    builds a prompt with their content as context, and sends the prompt to the LLM model to generate an answer.
    Returns the answer generated by the model.
    """

    if question is None or question.strip() == "":
        raise ValueError("A question must be provided to generate a RAG response.")

    # Initialize the Ollama client to interact with the LLM model
    client = Client(host=ollama_host_url)

    # Create or connect to a Chroma vector database for RAG
    vector_db = Chroma(
                    persist_directory = persist_directory,
                    embedding_function=OllamaEmbeddings(model=embedding_model, base_url=ollama_host_url), 
                    collection_name = collection_name
                    )
    
    # Get a retriever to fetch relevant documents from the database
    retriever = vector_db.as_retriever()

    # Retrieve the most relevant documents for the question
    docs = retriever.invoke(question)
    print("Documents fetched from database : " + str(len(docs)))

    # Merge the content of the documents into a single context
    context = "\n\n".join(doc.page_content for doc in docs)

    # Create the formatted prompt for the LLM model, including only the retrieved context
    formatted_prompt = f"""Answer the question based ONLY on the following context:
    {context}
    Question: {question}
    """

    # Send the prompt to the LLM model and get the answer
    response = client.chat(model = local_model, messages = [{'role': 'user', 'content': formatted_prompt}])
    return(response['message']['content'])

from ollama import Client
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from sentence_transformers import CrossEncoder
from sentence_transformers import SentenceTransformer, util
import logging
import config as config
import re

logging.basicConfig(
    level=config.LOGGING_LEVEL,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

def generate_rag_response(
                          ollama_host_url   = config.OLLAMA_HOST_URL, 
                          local_model       = config.LOCAL_MODEL, 
                          embedding_model   = config.EMBEDDING_MODEL, 
                          question          = None,
                          collection_name   = None, 
                          persist_directory = config.PERSIST_DIRECTORY,
                          chat_history      = None
                          ):
    
    """
    Function that implements a Retrieval-Augmented Generation (RAG) system.
    Given a local LLM model and a Chroma vector database, it retrieves the most relevant documents for a question,
    builds a prompt with their content as context, and sends the prompt to the LLM model to generate an answer.
    Returns the answer generated by the model.
    """

    if question is None or question.strip() == "":
        raise ValueError("A question must be provided to generate a RAG response.")

    # Initialize the Ollama client to interact with the LLM model
    client = Client(host=ollama_host_url)

    history_str = ""
    if chat_history:
        history_str = "\n".join([f"User: {msg['content']}" if msg['role']=='user' else f"Assistant: {msg['content']}" for msg in chat_history if msg['role'] in ['user', 'assistant']])

    logging.info("Generating search query for the question.")
    logging.debug("Chat History: '%s'", chat_history)
    # Generate a search query from the user's question
    query_generation_prompt = f"""You are an expert at rephrasing a user's question into a concise, keyword-based search query for a vector database. 
    Extract only the most critical entities and technical terms from the user's question, using the chat history for context.
    If the user's question is a follow-up, combine it with keywords from the history to create a specific query.
    Do not add any explanation or introductory text. Your output must be only the search query.

    ---
    
    **Multi-turn Example:**

    Chat History:
    User: explain what a for loop is in Python
    Assistant: A for loop is used for iterating over a sequence (that is either a list, a tuple, a dictionary, a set, or a string).

    User Question: "provide an example"
    Search Query: "Python for loop code example"

    ---

    **Single-turn Example:**

    Chat History:
    
    User Question: "Can you tell me all about how to use AWS S3 for storing large video files?"
    Search Query: "AWS S3 large video file storage"

    ---

    **Current Conversation:**

    Chat History:
    {history_str}

    User Question: "{question}"
    Search Query:"""

    response = client.chat(
        model=local_model, 
        messages=[{'role': 'user', 'content': query_generation_prompt}],
        options={'temperature': 0}
    )
    search_query = response['message']['content'].strip().strip('"')
    logging.info("Generated Search Query: '%s'", search_query)

    # Create or connect to a Chroma vector database for RAG
    vector_db = Chroma(
                    persist_directory = persist_directory,
                    embedding_function=OllamaEmbeddings(model=embedding_model, base_url=ollama_host_url), 
                    collection_name = collection_name
                    )
    
    # Get a retriever to fetch relevant documents from the database
    retriever = vector_db.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 8, "fetch_k": 20}
    )

    # Retrieve the most relevant documents for the question
    docs = retriever.invoke(search_query)
    logging.info("Documents fetched from database: %d", len(docs))

    # Log the retrieved documents for debugging purposes 
    logging.debug("--- CONTEXT RETRIEVED ---")
    for i, doc in enumerate(docs):
        logging.debug("--- Document %d ---\n%s\n--------------------", i+1, doc.page_content)
    logging.debug("--- END CONTEXT ---")

    # Sentence transformer layer to re-rank the retrieved documents based on their relevance to the search query
    model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')
    
    # Create pairs of (query, document) for the cross-encoder and get scores
    doc_scores = model.predict([[search_query, doc.page_content] for doc in docs])
    
    # Combine docs with their scores and sort
    scored_docs = sorted(zip(docs, doc_scores), key=lambda x: x[1], reverse=True)
    
    # Select top 5 documents
    docs = [doc for doc, score in scored_docs[:5]]
    logging.info("Documents re-ranked. Top documents selected: %d", len(docs))

    # Merge the content of the documents into a single context
    context = "\n\n".join(doc.page_content for doc in docs)

    messages = []
    if chat_history:
        messages.extend(chat_history)

    # Create the formatted prompt for the LLM model, including only the retrieved context
    # This prompt encourages a more natural, conversational response.
    formatted_prompt = f"""You are a helpful and friendly assistant.
Use the provided context below to answer the user's question.
Your goal is to have a natural conversation, so avoid starting your response with phrases like "Based on the context...".
If the information is not in the context, just say that you couldn't find the answer in the provided document.

---
Context:
{context}
---
Question: {question}
"""

    messages.append({'role': 'user', 'content': formatted_prompt})
    
    # Send the prompt to the LLM model and get the answer
    response = client.chat(model = local_model, messages = messages)
    return(response['message']['content'], docs)


def highlight_relevant_passages(answer, docs, num_snippets=3):
    """
    Function to highlight relevant passages from the retrieved documents in the generated answer.
    It searches for sentences in the documents that contain keywords from the answer and highlights them.
    Returns the answer with highlighted passages and their metadata.
    """

    model = SentenceTransformer('all-MiniLM-L6-v2')

    # Split retrieved documents into sentences
    answer_sentences = re.split(r'(?<=[.!?]) +', answer)

    # Split the documents into sentences
    doc_sentences = []
    for doc in docs:
        sentences = re.split(r'(?<=[.!?]) +', doc.page_content)
        doc_sentences.extend(sentences)

    # Compute similarity
    answer_embeddings = model.encode(answer_sentences, convert_to_tensor=True)
    doc_embeddings = model.encode(doc_sentences, convert_to_tensor=True)   
    cosine_scores = util.pytorch_cos_sim(answer_embeddings, doc_embeddings)

    # Find top relevant sentences
    highlighted_passages = set()
    for i in range(len(answer_sentences)):
        best_index = cosine_scores[i].argmax().item()
        highlighted_passages.add(doc_sentences[best_index])

    # Highlight the phrases in the context and include metadata
    highlighted_docs = []
    for doc in docs: 
        content = doc.page_content
        for snippet in highlighted_passages:
            if snippet in content:
                content = content.replace(snippet, f":orange-background[{snippet}]")
        highlighted_docs.append({
            "content": content,
            "metadata": doc.metadata if hasattr(doc, "metadata") else {}
        })

    return answer, highlighted_docs
